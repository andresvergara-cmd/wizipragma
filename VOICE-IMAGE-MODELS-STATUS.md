# Estado de Modelos de Voz e Im√°genes - CENTLI

## üìã Resumen Ejecutivo

**Modelos Especificados en Dise√±o**:
- ‚úÖ **Voz**: AWS Bedrock Nova Sonic (transcripci√≥n y s√≠ntesis)
- ‚úÖ **Im√°genes**: AWS Bedrock Nova Canvas (an√°lisis de im√°genes)

**Estado de Implementaci√≥n**:
- ‚ö†Ô∏è **Voz**: Especificado pero NO implementado (placeholder)
- ‚ö†Ô∏è **Im√°genes**: Especificado pero NO implementado (placeholder)

**Raz√≥n**: Enfoque del hackathon en funcionalidad core (texto + Action Groups). Voz e im√°genes quedaron como "Could Have" (baja prioridad).

---

## üéØ Modelos Especificados

### 1. Nova Sonic (Voz)

**Modelo**: `amazon.nova-sonic-v1:0`  
**Prop√≥sito**: Procesamiento de voz bidireccional  
**Capacidades**:
- Speech-to-Text (transcripci√≥n)
- Text-to-Speech (s√≠ntesis)
- Soporte para espa√±ol mexicano (es-MX)

**Especificado en**:
- `aidlc-docs/construction/agentcore-orchestration/nfr-requirements/tech-stack-decisions.md`
- `aidlc-docs/inception/requirements/requirements.md` (FR-012, FR-013)

**Configuraci√≥n Dise√±ada**:
```yaml
Language: es-MX (Mexican Spanish)
Voice: Neutral gender, professional tone
Speaking rate: Normal
Processing mode: Batch (< 3s latency target)
```

### 2. Nova Canvas (Im√°genes)

**Modelo**: `amazon.nova-canvas-v1:0`  
**Prop√≥sito**: An√°lisis de im√°genes  
**Capacidades**:
- Object detection (detecci√≥n de objetos)
- Text extraction / OCR (extracci√≥n de texto)
- Scene understanding (comprensi√≥n de escena)

**Especificado en**:
- `aidlc-docs/construction/agentcore-orchestration/nfr-requirements/tech-stack-decisions.md`
- `aidlc-docs/inception/requirements/requirements.md` (FR-015)

**Configuraci√≥n Dise√±ada**:
```yaml
Analysis types: Object detection, text extraction, scene understanding
Confidence threshold: 0.7 (70%)
Max objects: 10 per image
```

---

## üîç Estado de Implementaci√≥n

### Frontend (Unit 4)

#### Voz - `frontend/js/voice-manager.js`

**Estado**: ‚ö†Ô∏è Parcialmente implementado (captura de audio solamente)

**Implementado**:
- ‚úÖ Captura de audio usando MediaRecorder API
- ‚úÖ Grabaci√≥n en formato WebM
- ‚úÖ L√≠mite de 30 segundos
- ‚úÖ Conversi√≥n a base64
- ‚úÖ Env√≠o por WebSocket

**NO Implementado**:
- ‚ùå Integraci√≥n con Nova Sonic (transcripci√≥n)
- ‚ùå Reproducci√≥n de respuestas de voz
- ‚ùå S√≠ntesis de texto a voz

**C√≥digo Actual**:
```javascript
// voice-manager.js - Solo captura
async startRecording() {
  this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  this.mediaRecorder = new MediaRecorder(this.stream, {
    mimeType: 'audio/webm'
  });
  // ... captura y env√≠o
}
```

#### Im√°genes - `frontend/js/image-manager.js`

**Estado**: ‚ö†Ô∏è Parcialmente implementado (captura de imagen solamente)

**Implementado**:
- ‚úÖ Selecci√≥n de archivo de imagen
- ‚úÖ Validaci√≥n de formato (JPEG, PNG)
- ‚úÖ Compresi√≥n de imagen
- ‚úÖ Preview de imagen
- ‚úÖ Conversi√≥n a base64
- ‚úÖ Env√≠o por WebSocket

**NO Implementado**:
- ‚ùå Integraci√≥n con Nova Canvas (an√°lisis)
- ‚ùå Procesamiento de resultados de an√°lisis
- ‚ùå Visualizaci√≥n de objetos detectados

**C√≥digo Actual**:
```javascript
// image-manager.js - Solo captura y compresi√≥n
async compressImage(file) {
  // Comprime imagen a JPEG
  canvas.toBlob((blob) => resolve(blob), 'image/jpeg', 0.8);
}
```

### Backend (Unit 2)

#### Voz - `src_aws/app_message/app_message.py`

**Estado**: ‚ùå NO implementado (placeholder)

**C√≥digo Actual**:
```python
def process_voice_message(audio_data: str, session_id: str, user_id: str, connection_id: str) -> dict:
    """
    Process voice message (base64 audio).
    
    Note: Simplified for hackathon. Full implementation would:
    1. Decode base64 audio
    2. Invoke Nova Sonic for transcription
    3. Process text through AgentCore
    4. Invoke Nova Sonic for synthesis
    5. Return audio response
    """
    try:
        # Placeholder: Return text response
        return {
            "type": "TEXT",
            "content": "Voice processing not yet implemented. Please use text.",
            "metadata": {"timestamp": datetime.utcnow().isoformat()}
        }
    except Exception as e:
        print(f"ERROR: Voice processing failed: {str(e)}")
        return {"error": f"Voice processing failed: {str(e)}"}
```

**Mensaje al Usuario**: "Voice processing not yet implemented. Please use text."

#### Im√°genes - `src_aws/app_message/app_message.py`

**Estado**: ‚ùå NO implementado (placeholder)

**C√≥digo Actual**:
```python
def process_image_message(image_data: str, session_id: str, user_id: str, connection_id: str) -> dict:
    """
    Process image message (base64 image).
    
    Note: Simplified for hackathon. Full implementation would:
    1. Decode base64 image
    2. Upload to S3
    3. Invoke Nova Canvas for analysis
    4. Process results through AgentCore
    5. Return response
    """
    try:
        # Placeholder: Return text response
        return {
            "type": "TEXT",
            "content": "Image processing not yet implemented. Please use text.",
            "metadata": {"timestamp": datetime.utcnow().isoformat()}
        }
    except Exception as e:
        print(f"ERROR: Image processing failed: {str(e)}")
        return {"error": f"Image processing failed: {str(e)}"}
```

**Mensaje al Usuario**: "Image processing not yet implemented. Please use text."

---

## üìä Tabla de Estado

| Componente | Modelo Especificado | Frontend | Backend | Estado General |
|------------|---------------------|----------|---------|----------------|
| **Voz - Entrada** | Nova Sonic | ‚úÖ Captura | ‚ùå Transcripci√≥n | ‚ö†Ô∏è Parcial |
| **Voz - Salida** | Nova Sonic | ‚ùå Reproducci√≥n | ‚ùå S√≠ntesis | ‚ùå No implementado |
| **Imagen - Entrada** | Nova Canvas | ‚úÖ Captura | ‚ùå An√°lisis | ‚ö†Ô∏è Parcial |
| **Imagen - Salida** | Nova Canvas | ‚ùå Visualizaci√≥n | ‚ùå Procesamiento | ‚ùå No implementado |

---

## üèóÔ∏è Arquitectura Dise√±ada vs Implementada

### Arquitectura Dise√±ada (Documentaci√≥n)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Flujo de Voz Dise√±ado                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Frontend                    Backend                    AWS Bedrock
   ‚îÇ                           ‚îÇ                            ‚îÇ
   ‚îÇ 1. Captura audio          ‚îÇ                            ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                            ‚îÇ
   ‚îÇ                           ‚îÇ 2. Invoke Nova Sonic       ‚îÇ
   ‚îÇ                           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
   ‚îÇ                           ‚îÇ    (Speech-to-Text)        ‚îÇ
   ‚îÇ                           ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                           ‚îÇ 3. Texto transcrito        ‚îÇ
   ‚îÇ                           ‚îÇ                            ‚îÇ
   ‚îÇ                           ‚îÇ 4. Invoke AgentCore        ‚îÇ
   ‚îÇ                           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
   ‚îÇ                           ‚îÇ    (Claude 3.5 Sonnet)     ‚îÇ
   ‚îÇ                           ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                           ‚îÇ 5. Respuesta texto         ‚îÇ
   ‚îÇ                           ‚îÇ                            ‚îÇ
   ‚îÇ                           ‚îÇ 6. Invoke Nova Sonic       ‚îÇ
   ‚îÇ                           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
   ‚îÇ                           ‚îÇ    (Text-to-Speech)        ‚îÇ
   ‚îÇ                           ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                           ‚îÇ 7. Audio sintetizado       ‚îÇ
   ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                            ‚îÇ
   ‚îÇ 8. Reproduce audio        ‚îÇ                            ‚îÇ
```

### Arquitectura Implementada (Actual)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Flujo de Voz Actual                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Frontend                    Backend                    AWS Bedrock
   ‚îÇ                           ‚îÇ                            ‚îÇ
   ‚îÇ 1. Captura audio ‚úÖ       ‚îÇ                            ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                            ‚îÇ
   ‚îÇ                           ‚îÇ 2. ‚ùå NO IMPLEMENTADO      ‚îÇ
   ‚îÇ                           ‚îÇ    (Nova Sonic)            ‚îÇ
   ‚îÇ                           ‚îÇ                            ‚îÇ
   ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                            ‚îÇ
   ‚îÇ 3. Mensaje: "Not          ‚îÇ                            ‚îÇ
   ‚îÇ    implemented"           ‚îÇ                            ‚îÇ
```

---

## üéØ Raz√≥n de No Implementaci√≥n

### Contexto del Hackathon

**Priorizaci√≥n MoSCoW**:
- **Must Have**: Texto + AgentCore + Action Groups ‚úÖ
- **Should Have**: WebSocket + Frontend b√°sico ‚úÖ
- **Could Have**: Voz + Im√°genes ‚ö†Ô∏è (NO implementado)
- **Won't Have**: Autenticaci√≥n biom√©trica real ‚ùå

**Decisi√≥n del Equipo**:
1. Enfoque en funcionalidad core (texto)
2. Demostrar AgentCore + Action Groups
3. Voz e im√°genes como "demo de concepto" (captura solamente)
4. Tiempo limitado (8 horas)

**Documentado en**:
- `aidlc-docs/inception/user-stories/stories.md` (US-012, US-013, US-015 marcadas como "Could Have")
- `aidlc-docs/construction/plans/agentcore-orchestration-code-generation-plan.md` (voz e imagen como "simplified placeholders")

---

## üîß Implementaci√≥n Completa Requerida

### Para Nova Sonic (Voz)

#### Backend - `src_aws/app_message/app_message.py`

```python
import boto3
import base64

bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')

def process_voice_message(audio_data: str, session_id: str, user_id: str, connection_id: str) -> dict:
    """Process voice message through Nova Sonic."""
    try:
        # 1. Decode base64 audio
        audio_bytes = base64.b64decode(audio_data)
        
        # 2. Invoke Nova Sonic for transcription
        transcribe_response = bedrock_runtime.invoke_model(
            modelId='amazon.nova-sonic-v1:0',
            body=json.dumps({
                'audio': audio_data,
                'task': 'transcribe',
                'language': 'es-MX'
            })
        )
        
        transcribed_text = json.loads(transcribe_response['body'].read())['text']
        
        # 3. Process text through AgentCore
        agent_response = bedrock_agent.invoke_agent(
            agentId=AGENTCORE_ID,
            agentAliasId=os.environ.get('AGENTCORE_ALIAS_ID'),
            sessionId=session_id,
            inputText=transcribed_text
        )
        
        response_text = extract_agent_response(agent_response)
        
        # 4. Invoke Nova Sonic for synthesis
        synthesis_response = bedrock_runtime.invoke_model(
            modelId='amazon.nova-sonic-v1:0',
            body=json.dumps({
                'text': response_text,
                'task': 'synthesize',
                'language': 'es-MX',
                'voice': 'neutral'
            })
        )
        
        audio_output = json.loads(synthesis_response['body'].read())['audio']
        
        return {
            "type": "VOICE",
            "content": audio_output,  # base64 audio
            "text": response_text,     # texto para fallback
            "metadata": {"timestamp": datetime.utcnow().isoformat()}
        }
        
    except Exception as e:
        print(f"ERROR: Voice processing failed: {str(e)}")
        return {"error": f"Voice processing failed: {str(e)}"}
```

#### Frontend - `frontend/js/voice-manager.js`

```javascript
// Agregar reproducci√≥n de audio
playAudioResponse(base64Audio) {
  const audioBlob = this.base64ToBlob(base64Audio, 'audio/webm');
  const audioUrl = URL.createObjectURL(audioBlob);
  const audio = new Audio(audioUrl);
  audio.play();
}

base64ToBlob(base64, mimeType) {
  const byteCharacters = atob(base64);
  const byteNumbers = new Array(byteCharacters.length);
  for (let i = 0; i < byteCharacters.length; i++) {
    byteNumbers[i] = byteCharacters.charCodeAt(i);
  }
  const byteArray = new Uint8Array(byteNumbers);
  return new Blob([byteArray], { type: mimeType });
}
```

### Para Nova Canvas (Im√°genes)

#### Backend - `src_aws/app_message/app_message.py`

```python
def process_image_message(image_data: str, session_id: str, user_id: str, connection_id: str) -> dict:
    """Process image message through Nova Canvas."""
    try:
        # 1. Decode base64 image
        image_bytes = base64.b64decode(image_data)
        
        # 2. Upload to S3
        s3_key = f"images/{session_id}/{uuid.uuid4()}.jpg"
        s3_client = boto3.client('s3')
        s3_client.put_object(
            Bucket=ASSETS_BUCKET,
            Key=s3_key,
            Body=image_bytes,
            ContentType='image/jpeg'
        )
        
        # 3. Invoke Nova Canvas for analysis
        analysis_response = bedrock_runtime.invoke_model(
            modelId='amazon.nova-canvas-v1:0',
            body=json.dumps({
                'image': image_data,
                'tasks': ['object_detection', 'text_extraction', 'scene_understanding'],
                'confidence_threshold': 0.7,
                'max_objects': 10
            })
        )
        
        analysis_results = json.loads(analysis_response['body'].read())
        
        # 4. Format results for AgentCore
        analysis_text = format_analysis_for_agent(analysis_results)
        
        # 5. Process through AgentCore
        agent_response = bedrock_agent.invoke_agent(
            agentId=AGENTCORE_ID,
            agentAliasId=os.environ.get('AGENTCORE_ALIAS_ID'),
            sessionId=session_id,
            inputText=f"Analiza esta imagen: {analysis_text}"
        )
        
        response_text = extract_agent_response(agent_response)
        
        return {
            "type": "IMAGE_ANALYSIS",
            "content": response_text,
            "analysis": analysis_results,
            "s3_key": s3_key,
            "metadata": {"timestamp": datetime.utcnow().isoformat()}
        }
        
    except Exception as e:
        print(f"ERROR: Image processing failed: {str(e)}")
        return {"error": f"Image processing failed: {str(e)}"}

def format_analysis_for_agent(analysis_results):
    """Format Nova Canvas results for AgentCore."""
    objects = analysis_results.get('objects', [])
    text = analysis_results.get('text', '')
    scene = analysis_results.get('scene', '')
    
    return f"Objetos detectados: {', '.join([o['label'] for o in objects])}. Texto: {text}. Escena: {scene}"
```

---

## üìù Conclusi√≥n

**Respuesta a la pregunta**: "¬øQu√© modelo se est√° usando para voz e im√°genes?"

**Modelos Especificados**:
- ‚úÖ **Voz**: AWS Bedrock Nova Sonic (`amazon.nova-sonic-v1:0`)
- ‚úÖ **Im√°genes**: AWS Bedrock Nova Canvas (`amazon.nova-canvas-v1:0`)

**Estado de Implementaci√≥n**:
- ‚ö†Ô∏è **Voz**: Especificado en dise√±o, pero NO implementado en c√≥digo (placeholder)
- ‚ö†Ô∏è **Im√°genes**: Especificado en dise√±o, pero NO implementado en c√≥digo (placeholder)

**Funcionalidad Actual**:
- ‚úÖ Frontend captura audio e im√°genes
- ‚ùå Backend NO procesa con Nova Sonic/Canvas
- ‚ùå Usuario recibe mensaje: "Not yet implemented. Please use text."

**Raz√≥n**:
- Priorizaci√≥n del hackathon: Enfoque en texto + AgentCore + Action Groups
- Voz e im√°genes clasificadas como "Could Have" (baja prioridad)
- Tiempo limitado (8 horas)

**Para Implementar**:
- Agregar c√≥digo de invocaci√≥n de Nova Sonic (transcripci√≥n + s√≠ntesis)
- Agregar c√≥digo de invocaci√≥n de Nova Canvas (an√°lisis de im√°genes)
- Actualizar frontend para reproducir audio y mostrar resultados de an√°lisis
- Estimado: 4-6 horas adicionales de desarrollo

---

**Validado por**: AI Agent (Kiro)  
**Fecha**: 2026-02-17T18:30:00Z  
**Ambiente**: AWS us-east-1 (Cuenta: 777937796305)
